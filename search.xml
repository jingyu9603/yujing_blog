<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Ven图、聚类分析、相关性分析]]></title>
    <url>%2F2019%2F06%2F06%2FVen%E5%9B%BE%E3%80%81%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E3%80%81%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1、绘制Ven图123456789101112131415161718192021222324#载入相应的安装包library(VennDiagram)#进入到需要绘制Ven图的文件夹setwd("/data/YJ/YanNa/stringtie/trs/Pt/diff10/LPt")#filelist=list("Pt15","Pt17","Pt19","Pt2-","Pt20","Pt22","Pt3","Pt5","Pt6","Pt8","Pt9")#将需要绘制Ven图的样本的rowname汇集到一个list中filelist=list("LPt16","LPt24")for (Pt in filelist)&#123; files=list.files(path="/data/YJ/YanNa/stringtie/trs/Pt/diff10/LPt",pattern = Pt) trslist=list() image_name=paste(Pt,"_Ven",".tiff",sep="") for (i in files)&#123; base=strsplit(i,"_")[[1]][1] trslist[base]=read.table(i,header = T,colClasses = c("character","NULL","NULL")) &#125; #设置颜色 num=148 col=c() for (i in 1:length(trslist))&#123; col[i]=colors()[num] num=num+150 &#125; #绘制Ven图 venn.diagram(trslist,filename = image_name,col="transparent",fil=col,alpha=0.5,resolution = 500) 2、聚类分析一、K均值聚类K均值聚类又称为动态聚类，它的计算方法较为简单，也不需要输入距离矩阵。首先要指定聚类的分类个数N，随机取N个样本作为初始类的中心，计算各样本与类中心的距离并进行归类，所有样本划分完成后重新计算类中心，重复这个过程直到类中心不再变化。 二、层次聚类层次聚类又称为系统聚类，首先要定义样本之间的距离关系，距离较近的归为一类，较远的则属于不同的类。可用于定义“距离”的统计量包括了欧氏距离(euclidean)、马氏距离(manhattan)、 两项距离(binary)、明氏距离(minkowski)。还包括相关系数和夹角余弦。 层次聚类首先将每个样本单独作为一类，然后将不同类之间距离最近的进行合并，合并后重新计算类间距离。这个过程一直持续到将所有样本归为一类为止。在计算类间距离时则有六种不同的方法，分别是最短距离法、最长距离法、类平均法、重心法、中间距离法、离差平方和法。 123==!!注意==K均值聚类能处理比层次聚类更大的数据集。由于K均值聚类在开始要随机选择k个中心点，在每次调用函数时可能获得不同的方案。使用set.seed() 函数可以保证结果是可复制的。此外，聚类方法对初始中心值的选择也很敏感。 ==聚类分析的一般步骤== 选择合适的变量，这是第一步，也可能是最重要的一步，再高级的聚类方法也不能弥补聚类变量选择不好的问题。因此，首先选择你感觉可能对于识别和理解数据中不同观测值分组有重要影响的变量。 缩放数据，由于不同变量可能有不同的变化范围，以免那些变化范围大的变量对结果有不成比例的影响，常常需要在分析之前缩放数据。通常，将每个变量标准化为均值为0和标准差为1的变量。还有另外两种替代方法，比如每个变量被其最大值相除，或该变量减去它的平均值并除以变量的平均绝对偏差。实现代码如下：123df1 &lt;- apply(mydata, 2, function(x)&#123;(x-mean(x))/sd(x)&#125;)df2 &lt;- apply(mydata, 2, function(x)&#123;x/max(x)&#125;)df3 &lt;- apply(mydata, 2, function(x)&#123;(x – mean(x))/mad(x)&#125;) 在R中可以通过使用scale()函数实现df1代码片段的功能。 寻找异常点，通常聚类方法对于异常值比较敏感，对此有几种解决方法，通过outliers包中的函数来筛选异常单变量离群点，mvoutlier包中 有识别多元变量的离群点的函数。 计算距离，虽然不同的聚类算法差异很大，但是通常需要计算被聚类的实体之间的距离。两个观测之间最常用的距离度量是欧几里得距离，其它常用的还有曼哈顿距离、兰氏距离、非对称二元距离、最大距离和闵可夫斯基距离。在下文的论述当中默认采用欧几里得距离。两个观测值之间的欧几里得距离定义如下：用R中自带的dist()函数可以计算矩阵或数据框中所有行（观测值）之间的距离，dist()函数的格式为dist(x, method =)，默认为欧几里得距离。 观测值之间的距离越大，则异质性越大，距离也越远。 通常，欧几里得距离作为连续型数据的距离度量，对于其它类型的数据，可以使用cluster包中的daisy()函数获得包含任意二元、名义、有序或者连续属性组合的相异矩阵，并且cluster包中的其它函数可以用这种异质性进行聚类分析。 另外，当一个观测中的某一个变量变换范围太大，缩放数据有利于均衡各变量的影响。 选择聚类算法，对于不同类型和不同样本量的数据，针对性地选择不同的聚类算法，从大的方面讲有层次聚类和划分聚类，通常前者对于小样本来说很实用（如150个观测值或更少），而且这种情况下嵌套聚类更实用；后者能够处理更大的数据量，但是需要事先确定聚类的个数。然后，不管是层次聚类还是划分聚类，都需要一个特定的聚类算法，不同算法有着各自的优缺点，需要根据工程实践进行综合比选确定。 获 得一种或多种聚类方法，这是步骤5的延续。 确定类的数目，在确定最终聚类方案时必须确定类的数目。通常尝试不同的类型（比如2~K）并比较解的质量。NbClust包中的NbClust()函数30个不同的指标来帮助你进行选择，指标如此之多，可见这在聚类分析当中是个难题。 获得最终的聚类解决方案，类的个数确定以后，就可以提取出子群，形成最终的聚类方案。 结果可视化，可视化可以帮助判定聚类方案的意义和用处，层次聚类的结果通常表示为一个树状图。划分的结果通常利用可视化双变量聚类图来表示。 解读类，聚类方案确定以后，结果也出来了，必须对其进行解读。比如，一个类中的观测值有何相似之处，不同类之间有何不同，这一步通常通过获得类中每个变量的汇总统计来完成。对于连续数据，每一类中变量的均值和中位数会被计算出来。对于混合数据（数据中包含分类变量），结果中将返回各类的众数或类别分布。 验证结果，对于聚类方案的验证相当于确认下，如果采用不同的聚类方法或者不同样本，是否会产生相同的类。fpc、clv和clValid包包含了评估聚类解的稳定性的函数。 12345678910111213141516171819#安装并加载包 #使用k-means聚类所需要的包：factoextra和cluster site="https://mirrors.tuna.tsinghua.edu.cn/CRAN" pakeags_list=c("factoextra","cluster","NbClust") for(p in pakeags_list)&#123; if(!suppressWarnings(suppressMessages(require(p, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE))))&#123; install.packages(p, repos=site) suppressWarnings(suppressMessages(library(p, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE))) &#125; &#125; if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager") BiocManager::install("ConsensusClusterPlus", version = "3.8") #载入相关的程序包 library(ConsensusClusterPlus) library(factoextra) library(cluster) library(NbClust) library(reshape2) 123456789101112131415161718192021222324252627282930#载入数据data&lt;-read.table("Pt_RPt_LW.txt",header=T)#根据rownames去掉数据中的重复行index&lt;-duplicated(data$trs)data&lt;-data[!index,]rownames(data)&lt;-data$trsdata&lt;-data[,-1]#t()函数对数据进行转置，形成，行为基因名，列为样本名称的数据集tdata&lt;-as.matrix(t(data))#去掉数据集中的NA函数my_data&lt;-na.omit(tdata)#在聚类之前，进行不要的数据检查即数据描述性统计，如平均值、标准差stats=data.frame(Min=apply(data,2,min), Med=apply(data,2,median), Mean=apply(data,2,mean), SD=apply(data,2,sd), Max=apply(data,2,max) )stats=round(stats,1)#保留小数点最后一位#数据标准化和评估#变量有很大的方差及均值时需要进行标准化df=scale(my_data)#数据集群性评估，使用get_clust_tendency()计算Hopkins统计量res=get_clust_tendency(df,40,graph=TRUE)res$hopkins_stat #Hopkins统计量的值&lt;0.5,表明数据是高度可聚合的res$plot 使用Kmean函数算法的简单描述如下： 选择K个中心点（随机选择K行）； 把每个数据点分配到离它最近的中心点； 重新计算每类中的点到该类中心点距离的平均值； 分配每个数据到它最近的中心点； 重复步骤3和步骤4，直到所有的观测值不再被分配或是达到最大的迭代次数。 从算法上可以看出，初始中心值的选择对于结果也非常敏感，kmeans()函数有一个nstart选项可以尝试多种初始配置并输出最好的一个结果。举个例子，加上nstart=25会生成25个初始配置，一般来说，我们推荐使用这种方法。 123456#计算最适合的K-mean聚类的分类数 set.seed(123) gap_stat=clusGap(df,FUN=kmeans,nstart=25,K.max = 10,B=500) fviz_gap_stat(gap_stat) km.res=kmeans(df,2,nstart = 25) fviz_cluster(km.res,df) eclust():增强的聚类分析与其他聚类分析包相比，eclust()有以下优点： 简化了聚类分析的工作流程， 可以用于计算层次聚类和分区聚类， eclust()自动计算最佳聚类簇数。 自动提供Silhouette plot，可以结合ggplot2绘制优美的图形123456#使用eclust()的K均值聚类res.km&lt;-eclust(df,"kmeans")fviz_gap_stat(res.km$gap_stat)#利用eclust()做层次聚类res.hc=eclust(df,"hclust")fviz_dend(res.hc,rect = TRUE) 利用hclust()做层次聚类 层次聚类的算法如下： 定义每一个观测值（行或单元）为一类； 计算每类和其他各类的距离； 把距离最短的两类合并成一类，减少一个类的个数； 重复步骤2和3，直到包含所有观测值的类合并成单个类为止。 不同的层次聚类算法主要区别在于它们对类的定义不同，常见的五种聚类方法及其距离定义如下层次聚类方法的实现格式如下： 1hclust(d, method=) 其中d是通过dist() 函数产生的距离矩阵， 并且方法包括 “single” 、”complete” 、”average” 、”centroid”和”ward”。 12345678#先求样本之间两两相似性result&lt;-dist(df,method="euclidean")#产生层次结构result_hc&lt;-hclust(d=result,method = "ward.D2")#初步结果fviz_dend(result_hc,cex=0.3)#可视化展示fviz_dend(result_hc,k=2,cex=0.3,k_colors = c("#E7B800", "#00AFBB"),color_labels_by_k = TRUE,rect = TRUE) 利用NbCluster()函数首先选择最适合的分类k12##利用NbCluster()函数首先选择最适合的分类k res_nbclust&lt;-NbClust(df,distance = "euclidean",min.nc = 2,max.nc = 10,method = "complete",index = "all") 3、相关性分析相关系数的显著性水平使用Hmisc包，计算矩阵相关系数及其对应的显著性水平 1234567891011121314151617181920212223242526272829303132333435363738394041424344library(Hmisc)res&lt;-rcorr(as.matrix(mtcars))res mpg cyl disp hp drat wt qsec vs am gear carbmpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00n= 32 P mpg cyl disp hp drat wt qsec vs am gear mpg 0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054cyl 0.0000 0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042disp 0.0000 0.0000 0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010hp 0.0000 0.0000 0.0000 0.0100 0.0000 0.0000 0.0000 0.1798 0.4930drat 0.0000 0.0000 0.0000 0.0100 0.0000 0.6196 0.0117 0.0000 0.0000wt 0.0000 0.0000 0.0000 0.0000 0.0000 0.3389 0.0010 0.0000 0.0005qsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389 0.0000 0.2057 0.2425vs 0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000 0.3570 0.2579am 0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570 0.0000gear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000 carb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290 carb mpg 0.0011cyl 0.0019disp 0.0253hp 0.0000drat 0.6212wt 0.0146qsec 0.0000vs 0.0007am 0.7545gear 0.1290carb 提取矩阵相关及其P值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465CorMatrix &lt;- function(cor,p) &#123;+ ut &lt;- upper.tri(cor) + data.frame(row = rownames(cor)[row(cor)[ut]] ,+ column = rownames(cor)[col(cor)[ut]], + cor =(cor)[ut], + p = p[ut] )+ &#125;res &lt;- rcorr(as.matrix(mtcars))&gt; CorMatrix (res$r, res$P) row column cor p1 mpg cyl -0.85216196 6.112688e-102 mpg disp -0.84755138 9.380328e-103 cyl disp 0.90203287 1.803002e-124 mpg hp -0.77616837 1.787835e-075 cyl hp 0.83244745 3.477861e-096 disp hp 0.79094859 7.142679e-087 mpg drat 0.68117191 1.776240e-058 cyl drat -0.69993811 8.244636e-069 disp drat -0.71021393 5.282022e-0610 hp drat -0.44875912 9.988772e-0311 mpg wt -0.86765938 1.293958e-1012 cyl wt 0.78249579 1.217567e-0713 disp wt 0.88797992 1.222311e-1114 hp wt 0.65874789 4.145827e-0515 drat wt -0.71244065 4.784260e-0616 mpg qsec 0.41868403 1.708199e-0217 cyl qsec -0.59124207 3.660533e-0418 disp qsec -0.43369788 1.314404e-0219 hp qsec -0.70822339 5.766253e-0620 drat qsec 0.09120476 6.195826e-0121 wt qsec -0.17471588 3.388683e-0122 mpg vs 0.66403892 3.415937e-0523 cyl vs -0.81081180 1.843018e-0824 disp vs -0.71041589 5.235012e-0625 hp vs -0.72309674 2.940896e-0626 drat vs 0.44027846 1.167553e-0227 wt vs -0.55491568 9.798492e-0428 qsec vs 0.74453544 1.029669e-0629 mpg am 0.59983243 2.850207e-0430 cyl am -0.52260705 2.151207e-0331 disp am -0.59122704 3.662114e-0432 hp am -0.24320426 1.798309e-0133 drat am 0.71271113 4.726790e-0634 wt am -0.69249526 1.125440e-0535 qsec am -0.22986086 2.056621e-0136 vs am 0.16834512 3.570439e-0137 mpg gear 0.48028476 5.400948e-0338 cyl gear -0.49268660 4.173297e-0339 disp gear -0.55556920 9.635921e-0440 hp gear -0.12570426 4.930119e-0141 drat gear 0.69961013 8.360110e-0642 wt gear -0.58328700 4.586601e-0443 qsec gear -0.21268223 2.425344e-0144 vs gear 0.20602335 2.579439e-0145 am gear 0.79405876 5.834043e-0846 mpg carb -0.55092507 1.084446e-0347 cyl carb 0.52698829 1.942340e-0348 disp carb 0.39497686 2.526789e-0249 hp carb 0.74981247 7.827810e-0750 drat carb -0.09078980 6.211834e-0151 wt carb 0.42760594 1.463861e-0252 qsec carb -0.65624923 4.536949e-0553 vs carb -0.56960714 6.670496e-0454 am carb 0.05753435 7.544526e-0155 gear carb 0.27407284 1.290291e-01 corrplot 12345install.packages("corrplot")library(corrplot)cor&lt;-cor(data,method = "pearson")corrplot(cor,order = "hclust",method="shade",addrect = 2,tl.cex=0.25,tl.col = "blue")write.table(cor,file="Pt_RLW_FPKM_cor.txt",sep=" ",row.names = TRUE,col.names = TRUE,quote=F) scatter plots 12library(PerformanceAnalytics)chart.Correlation(mtcars,histogram = TRUE,pch=19)]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>Ven图 聚类分析 相关性分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用biomaRT转换ID]]></title>
    <url>%2F2019%2F06%2F06%2F%E4%BD%BF%E7%94%A8biomaRT%E8%BD%AC%E6%8D%A2ID%2F</url>
    <content type="text"><![CDATA[12library(biomaRt)#显示包含的数据库及其版本 显示包含的数据库及其版本, 123456&gt; listMarts() biomart version1 ENSEMBL_MART_ENSEMBL Ensembl Genes 942 ENSEMBL_MART_MOUSE Mouse strains 943 ENSEMBL_MART_SNP Ensembl Variation 944 ENSEMBL_MART_FUNCGEN Ensembl Regulation 94 本次使用选用ENSEMBL_MART_ENSEMBL这个数据库 1ensembl=useMart("ENSEMBL_MART_ENSEMBL") 显示该数据库包含的子数据库 1head(listDatasets(ensembl)) 1234567847 ggallus_gene_ensembl48 ggorilla_gene_ensembl49 gmorhua_gene_ensembl50 hburtoni_gene_ensembl51 hcomes_gene_ensembl52 hfemale_gene_ensembl53 hmale_gene_ensembl54 hsapiens_gene_ensembl 选择查询的数据库及其相关数据集 1ensembl&lt;-useDataset("hsapiens_gene_ensembl",mart=ensembl) 查询filter函数包含的属性，这里filter函数代表的输入（即已知信息）的属性 12filters&lt;-listFilters(ensembl)head(filters) 显示attributes函数的属性，这里attributes函数要选择需要查询的属性 12attributes&lt;-listAttributes(ensembl)head(attributes) 1234567 name description page1 ensembl_gene_id Gene stable ID feature_page2 ensembl_gene_id_version Gene stable ID version feature_page3 ensembl_transcript_id Transcript stable ID feature_page4 ensembl_transcript_id_version Transcript stable ID version feature_page5 ensembl_peptide_id Protein stable ID feature_page6 ensembl_peptide_id_version Protein stable ID version feature_page 123456789rownames_old=rownames(Pt_union_20)##去掉id后面的小数点，也就是版本号ens=c()for(i in rownames_old)&#123; ens[i]=strsplit(i,"\\.")[[1]][1]&#125;names(ens)&lt;-NULL#利用getBM()函数进行ID转换gene_symbol=getBM(attributes = c("ensembl_transcript_id","hgnc_symbol"),filters = "ensembl_transcript_id",values = ens,mart=ensembl) 1234##将ID转换过程中，在数据库中没有找到对应ID的空白值转换为“none字符”for(j in which(gene_symbol[,2]==""))&#123; gene_symbol[j,2]="none"&#125; 12345678910#将ID与genesymbol连起来，形成新的IDrownames_new=c()for(i in rownames_old)&#123; trs_id=strsplit(i,"\\.")[[1]][1] print(trs_id) index=match(trs_id,gene_symbol$ensembl_transcript_id) print(index) rownames_new[i]&lt;-paste(i,gene_symbol[index,2],sep=",")&#125;names(rownames_new)&lt;-NULL 当需要转换的id中既有gene_id又有transcrption_id时，可以采用以下的方法： 123456789101112131415161718192021222324252627282930313233343536373839rownames_old=rownames(Pt_union_top100)g_id=c()t_id=c()for(i in rownames_old)&#123; id=strsplit(i,"\\.")[[1]][1] if(grepl("G",id))&#123; g_id[i]=id &#125; else&#123; t_id[i]=id &#125;&#125;names(g_id)&lt;-NULLnames(t_id)&lt;-NULLgene_symbol_t=getBM(attributes = c("ensembl_transcript_id","hgnc_symbol"),filters = "ensembl_transcript_id",values = t_id,mart=ensembl)gene_symbol_g=getBM(attributes = c("ensembl_gene_id","hgnc_symbol"),filters="ensembl_gene_id",values = g_id,mart=ensembl)for(j in which(gene_symbol_t[,2]==""))&#123; gene_symbol_t[j,2]="none"&#125;for(j in which(gene_symbol_g[,2]==""))&#123; gene_symbol_g[j,2]="none"&#125;rownames_new=c()for(i in rownames_old)&#123; trs_id=strsplit(i,"\\.")[[1]][1] print(trs_id) if(grepl("G",trs_id))&#123; index=match(trs_id,gene_symbol_g$ensembl_gene_id) rownames_new[i]&lt;-paste(i,gene_symbol_g[index,2],sep=";") &#125; else&#123; index=match(trs_id,gene_symbol_t$ensembl_transcript_id) rownames_new[i]&lt;-paste(i,gene_symbol_t[index,2],sep=";") &#125;&#125;names(rownames_new)&lt;-NULLrownames(Pt_union_top100)&lt;-rownames_new]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>biomaRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo+Netlify搭建个人博客]]></title>
    <url>%2F2019%2F06%2F06%2F%E4%BD%BF%E7%94%A8Hexo%2BNetlify%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[个人博客搭建准备工作需要安装Git和Node.js Git的安装 下载windows最新版，Git-2.21.0-64-bit.exe，在git使用过程中遇到下面的两个错误1、git报错： 1fatal: bad config line 1 in file C:/Users/JIANGXIAOLIANG/.gitconfig 解决方案：找到提示的目录，然后删掉.gitconfig文件然后在重新配置用户名和邮箱，输入下面的命令： 12$ git config --global user.name "用户名"$ git config --global user.email "邮箱" 我在使用的时候没有重新配置用户好像也可以正常使用 2、使用hexo 创建目录时 12345678C:\Users\Administrator&gt;hexo init F:/blogINFO Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitCloning into 'F:\blog'...fatal: unable to access 'https://github.com/hexojs/hexo-starter.git/': error setting certificate verify locations: CAfile: E:/Using Software/Git/mingw32/ssl/certs/ca-bundle.crt CApath: noneWARN git clone failed. Copying data insteadWARN Failed to install dependencies. Please run 'npm install' manually! 解决方案 1git config --system http.sslverify false Node.js的安装 在官网下载最新版本的node-v10.16.0-x64.msi，像安装一般软件一样安装Node.js,在安装完以后，将Node的路径添加到计算的环境变量中，然后验证Node.js的版本 12C:\Users\Administrator&gt;node --versionv10.16.0 安装Hexo打开git软件下的git-bash.exe终端，在终端中使用npm安装Hexo 1npm install -g hexo-cli 安装完成后创建项目文件夹 123hexo init F:/blog# 进入刚刚创建的文件夹cd F:/blog 建站1234567891011#通过npm完成Hexo初始化npm install#网站的雏形已经建好了，可以通过hexo服务器来预览成果hexo server# 可以前往 http://localhost:4000/ 访问刚刚建立的最新网站# 新建博客文章hexo new “我的最新日志”# 此时已经可以发现在文件夹./source/_posts下面多了一个我的最新日志.md文件#下一步，生产静态文件hexo generate# 如果hexo服务器还在运行中的话，刷新网页，可以看见刚刚创建的博客文章 网站发布前的准备工作有一个细节值得一提，在默认情况下，Hexo将生成的网站内容存储至public文件夹。鉴于我们不需要对该文件夹的内容进行版本控制，我们可将该文件夹添加至.gitignore文档中: 1echo "/public" &gt;&gt; .gitignore 接下来将内容推送到习惯使用的代码托管服务，一般将其托管到GitHub新建仓库 首先，在GitHub上新建仓库。为了避免出错，在新建仓库时，请不要在创建Initialize this repository with a README前打勾，Add .gitignore和Add a license处请选择None。 鉴于我们的demo基于Hexo和Netlify，在Repository name处填写hexo_netlify来命名仓库。 打开你的电脑终端，切换至你的项目所在的本地文件夹路径： 1234567891011cd /F/blog# 初始化仓库git init# 该命令将创建一个名为.git的子目录，其中包含了你初始化的Git仓库中所需的文件，这些文件是Git仓库的核心。此时，我们仅作了一个初始化的操作，你的项目文件还未被跟踪。 # 通过git add 来实现对指定文件的跟踪，然后执行git commit提交:git add .git commit -m "initial commit"# 回到之前我们创建GitHub仓库完成的页面，复制远程仓库链接，在终端输入git remote add origin git@github.com:jingyu9603/hexo_netlify.git用以下指令推送本地仓库内容至GitHubgit push origin master 在上一步的使用中可能出现： 12$ ssh -T git@github.comgit@github.com: Permission denied (publickey). 解决方法 123456789101112# 创建密钥ssh-keygen -t rsa -b 4096 -C "11yj3312@gmail.com"# 当出现“Enter a file in which to save the key”，按回车键就可以了；出现“nter passphrase (empty for no passphrase)”，也是按回车就可以了# 开启ssh-agenteval $(ssh-agent -s)# 添加密钥到ssh中 ssh-add ~/.ssh/id_rsa# 将密钥添加到github的ssh中,复制id_rsa.pub中的内容添加到github count setting的SSH and GPG keys 中$ cat /c/Users/Administrator/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC5piJ696hRcYzvP0OsGdJHXK+OM1fqUMN2cNSlQJ3GbJYnZn2yuJ5+NTy+tXXtieuqmI4m0b7kB1z0kat8i5r/ML9yk8lvIDekZlKBIiSoW8WcgOGr+mhsJJ6tcCwMOXNOcSYA3dI6lQh+ElVbpRyIffr0jhxZVSH792bxcr6FTu8lh0YvHZD0k/N6NcsrZt6yklFJH+o4yB/mxRdkzdAANNI6TuJad+Jght51Djcsd3lY+4tGqRTYSpohYrLofKREvE9+ZxyWv+Nsf+C9I/CTF6h0J14XRnJo7KNx1sXVsVsmBru5BN0rm0M1IuevbJjmJNsWoT6TYICD2WE6tAgigbvjAx5eCZLdqYXmP8J9TqvXI/heQvKU9BhfJETu8ewij8mWA1vrneorNiofQ9ZiIH0OC5sxJQQWL380GeCvzvP4DTJTj+g8GXvmwW/FnPOEVqliE3nKjzg9GghjBzNu1RHSxl5DxBWi+Rkj0vuQYqmbZ1xDEnrI2fD3GLBFu/ew8jZ/lw7Lk2VSA0oDIaFEhWg9wxSh0j8uUqC6RcSF4idpqIwFYCu8Gu1oP0swEkYc/vsIA5P1aPuqeYrUwbKqCJfiCxwd+SJkKsceNJBZm6g3yBjf3hi8DB/pIIbUk0fhwGZYTo7+YItxZQY4haMetXPClzm93QRg1Cx8134a2Q== 11yj3312@gmail.com# 在SSH and GPG keys 中点击New SSH key，title可以随便命名，将上面的内容复制到其中 通过上面的步骤，项目已经推送大GitHub的master分支下面了，接下来我们对hexo进行一些配置：在hexo的根目录下面，用vim打开——config.yml文件： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git #部署方式 repository: git@github.com:jingyu9603/yujing_blog.git #关联github仓库 branch: run-page #部署分支 在这里部署的分支用于后面更新博客使用在配置好以后，我们再执行下面的操作： 12345678hexo clean #清理各种缓存和旧文件hexo g #生成静态文件hexo d # 将public目录同步到GitHub## 在进行部署时可能会遇到下面的错误ERROR Deployer not found: gi## 这是由于我们缺少了一个依赖，安装如下:npm install hexo-deployer-git --save 再执行一下hexo d操作，我们会发现在我们github项目中多了一个run-page的分支 发布网站 注册一个Netlify账号，Netlify提供邮箱注册和包括GitHub、GitLab和Bitbucket在内的第三方验证登陆，在这里以GitHub为例 注册好以后，进入页面，点击‘New sit from Git’ 点击GitHub，关联Netlify和github的仓库，首次关联时点击Authorize netlify同意授权后，Netlify可以有权访问你在GitHub上的仓库内容了，授权完成以后，就可以选择之前建立的hexo_netlify仓库 选择run-page分支，在Basic build setting 选项中，两个条目清空，最后点击Deploysit。Netlify就可以构建并发布网站内容了 构建完成以后，Netlify会在网站发布成功的同时提供给你一个*.netlify.com为后缀以及随机生成的英文名为前缀的子域名。假如你不喜欢Netlify给你的前缀，点击Site settings,在里面可以修改。 博客优化安装主题在网页上寻找自己喜欢的主题，主题的安装过程很简单，以next主题为例子，在官网有安装步骤： 12mkdir themes/next curl -s https://api.github.com/repos/iissnan/hexo-theme-next/releases/latest | grep tarball_url | cut -d '"' -f 4 | wget -i - -O- | tar -zx -C themes/next --strip-components=1 启用主题修改站点的配置文件_config.yml 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next next主题下面还分了四种不同的主题： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 Gemini - 左侧网站信息及目录，块+片段结构布局 在主题配置文件，搜索 scheme 关键字。 你会看到有四行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除就可以了 使用Valine插件使得Hexo博客具有评论功能可参见SmartSi的blog 获取appid和appkey 请先登录或注册 LeanCloud, 进入控制台后点击左下角创建应用，选择免费的开发版即可。注意右上角有几个节点，可以就近选择。 应用创建好以后，进入刚刚创建的应用，选择左下角的设置&gt;应用Key，然后就能看到你的appid和appkey了： Hexo中的开启和设置 在配置文件中修改代码1234567891011# Valinevaline: enable: true appid: ## 之前获得的id appkey: ## 之前获得key notify: false #新的留言是否需要通知 verify: false #留言是否需要验证 placeholder: 欢迎留言！在这里说出你的想法！ avater: mm guest_info: nick,mail pageSize: 10 添加搜索功能 安装hexo-generator-searchdb 插件 1npm install hexo-generator-searchdb --save 打开 站点配置文件 找到Extensions在下面添加 123456# 搜索search: path: search.xml field: post format: html limit: 10000 打开 主题配置文件 找到Local search，将enable设置为true 添加阅读全文按钮如果只想要显示文章的部分内容，只需要在文章的合适位置添加： 1&lt;!--more--&gt; 设置网站缩略图标我的设置方法是这样的：把你的图片（png或jpg格式，不是favicon.ico）放在themes/next/source/images里，然后打开 主题配置文件 找到favicon，将small、medium、apple_touch_icon三个字段的值都设置成/images/图片名.jpg就可以了，其他字段都注释掉。 头像设置打开 主题配置文件 找到Sidebar Avatar字段 12# Sidebar Avataravatar: /images/header.jpg 设置代码高亮在站点配置文件中，搜索highlight 12345highlight: enable: true line_number: true auto_detect: true tab_replace: 文字自动检测默认不启动，所以改成true使其起作用。再到主题配置文件中：找到highlight_theme: normal，注释显示有五种显示主题可用，分别是： normal night night eighties night blue night bright选择什么要看个人审美了。]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R中GEO数据的预处理]]></title>
    <url>%2F2019%2F06%2F05%2FR%E4%B8%ADGEO%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[获得芯片探针与gene的对应关系的三种方法： 在基因芯片的厂商的官网直接下载 从NCBI里面下载文件来解析 直接使用Bioconductor包 以下是直接使用Biocondutor包的流程： 获取平台与Bioconductor包之间的对应关系 12345678910library(BiocManager)BiocManager::install("GEOmetadb")install.packages("RSQLite")devtools::install_github("ggrothendieck/sqldf")library(GEOmetadb)library(RSQLite)library(sqldf)getSQLiteFile()con&lt;-dbConnect(RSQLite::SQLite(),"GEOmetadb.sqlite")gplToBioc&lt;-dbGetQuery(con,'select gpl,bioc_package,title from gpl where bioc_package is not null') 下载所需要的包 1234567for(i in 1:nrow(gplToBioc))&#123; platform=gplToBioc[i,1] platformDB=paste(platform,".db",sep = "") if(platformDB %in% rownames(installed.packages()) ==FALSE)&#123; BiocManager::install(platformDB) &#125;&#125; 批量获得芯片探针与gene的对应关系 1234567891011121314151617library(hgu133a.db)library(GEOquery)#下载GEO数据集GEO&lt;-"GSE24673"if(!file.exists(GEO))&#123; gset&lt;-getGEO(GEO,destdir = ".",getGPL = F,AnnotGPL = F) save(gset,file=GEO)&#125;data&lt;-gset[[1]]data&lt;-exprs(data)ids&lt;-toTable(hgu133aSYMBOL)data&lt;-data[ids$probe_id,]ids$median&lt;-apply(data,1,median)ids&lt;-ids[order(ids$symbol,ids$median,decreasing = T),]ids&lt;-ids[!duplicated(ids$symbol),]data&lt;-data[ids$probe_id,]rownames(data)&lt;-ids$symbol]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>GEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Whole-exome analysis]]></title>
    <url>%2F2019%2F06%2F05%2FWhole-exome_analysis%2F</url>
    <content type="text"><![CDATA[软件准备在相关软件官网下载最新版本的软件 bwa-0.7.17.tar.bz2 gatk-4.1.2.0.zip picard-2.20.0-0.tar.bz2 samtools-1.9.tar.bz2 原始数据质量控制分析数据质量的几个方面： read各个位置的碱基质量分布 碱基的总体质量分布 read各个位置上碱基分布比例看，目的是为了分析碱基的分离程度 GC含量分布 read各个位置的N含量 read是否还包含测序的接头序列 read重复率，这个是实验的扩增所引入的 需要的软件 FastQC，一个java程序，可以很好的帮助我们理解测序数据的质量情况，唯一不足的就是图太丑了 MultiQC,一个基于Python的小工具，能够将测序数据的多个QC结果整合成一个HTLM网页交互式报告，同时也能导出pdf Trimmomatic（cutadapt、sickle、seqtk）等可用于切除接头序列和read的低质量序列 全外显子组分析（WES）需要的工具： bwa samtools bgzip GATK 4.0 GATK 3.x Picard 数据预处理序列比对 为参考基因组构建索引 1bwa index hg38.fa 将read比对到参考基因组 1bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz &gt; sample_name.sam -t，线程数，我们在这里使用4个线程 ；-R 接的是Read Group的字符串信息，这是一个非常重要的信息，以@RG开头，它是用来将比对的read进行分组的。这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要。在Read Group中，有如下几个信息非常重要： 1）ID，这是Read Group的分组ID，一般设置为测序的lane ID（不同lane之间的测序过程认为是独立的），下机数据中我们都能看到这个信息的，一般都是包含在fastq的文件名中； 2）PL，指的是所用的测序平台，这个信息不要随便写！特别是当我们需要使用GATK进行后续分析的时候，更是如此！这是一个很多新手都容易忽视的一个地方，在GATK中，PL只允许被设置为：ILLUMINA,SLX,SOLEXA,SOLID,454,LS454,COMPLETE,PACBIO,IONTORRENT,CAPILLARY,HELICOS或UNKNOWN这几个信息。 3）SM，样本ID，同样非常重要，有时候我们测序的数据比较多的时候，那么可能会分成多个不同的lane分布测出来，这个时候SM名字就是可以用于区分这些样本； 4）LB，测序文库的名字，这个重要性稍微低一些，主要也是为了协助区分不同的group而存在。文库名字一般可以在下机的fq文件名中找到，如果上面的lane ID足够用于区分的话，也可以不用设置LB。 为了方便后续的分析，一般在输出文件时，将其转换为bam文件 1bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz | samtools view -S -b - &gt; sample_name.bam 排序（sort）第一步的比对是按照FASTQ文件的顺序把read逐一定位到参考基因组上之后，随即就输出了，它不会也不可能在这一步里面能够自动识别比对位置的先后位置重排比对结果。因此，比对后得到的结果文件中，每一条记录之间位置的先后顺序是乱的，我们后续去重复等步骤都需要在比对记录按照顺序从小到大排序下来才能进行，所以这才是需要进行排序的原因。 1time samtools sort -@ 4 -m 4G -O bam -o sample_name.sorted.bam sample_name.bam -@，用于设定排序时的线程数，我们设为4；-m，限制排序时最大的内存消耗，这里设为4GB；-O 指定输出为bam格式；-o 是输出文件的名字， 标记重复序列（删除重复序列）我们使用Picard来完成这个事情 12345##标记重复序列java -jar picard.jar MarkDuplicates \ I=sample_name.sorted.bam \ O=sample_name.sorted.markdup.bam \ M=sample_name.markdup_metrics.txt 这里只把重复序列在输出的新结果中标记出来，但不删除。如果我们非要把这些序列完全删除的话可以这样做： 12345java -jar picard.jar MarkDuplicates \ REMOVE_DUPLICATES=true \ I=sample_name.sorted.bam \ O=sample_name.sorted.markdup.bam \ M=sample_name.markdup_metrics.txt 在后面的分析中，在Somatic SNVs+Indels的过程中使用标记的bam的文件，并且使用GATK4.0的主流pipline；在Detect MSI（微卫星不稳定性）的过程中使用GATK3.x的过程 Somatic SNVs+Indels重新矫正碱基质量值（BQSR）分为两步: 123456789101112 gatk BaseRecalibrator \ -I my_reads.bam \ -R reference.fasta \ --known-sites sites_of_variation.vcf \ --known-sites another/optional/setOfSitesToMask.vcf \ -O recal_data.table## 用到的已知变异位点common_all_20180418_dbsnp_151_hg38.vcfHomo_sapiens_assembly38.known_indels.vcfMills_and_1000G_gold_standard.indels.hg38.vcf下载地址（GATK的bundle以及1000genomic官网）：ftp://ftp.broadinstitute.org/bundle/hg38/https://ftp.ncbi.nlm.nih.gov/snp/organisms/ 12345gatk ApplyBQSR \ -R reference.fasta \ -I input.bam \ --bqsr-recal-file recalibration.table \ -O output.bam 查看测序深度 1samtools depth bamfile | awk '&#123;sum+=$3&#125; END &#123; print "Average = "sum/NR&#125;' 使用qualimap来查看比对好的bam文件的质量在官网下载使用qualimap 123456wget https://bitbucket.org/kokonech/qualimap/downloads/qualimap_v.2.12.zipunzip qualimap_v.2.12.zipcd qualimap_v.2.12./qualimap -h# 使用qualimap时 一般需要添加内存参数,对于外显子数据，需要添加关于外显子区域的bed文件qualimap --java-mem-size=8G bamqc -bam in.bam -gff exome.bed 使用旧版的mutects来call somatic mutation（现在GATK网站已经不推荐了）** CreateSomaticPanelOfNormals ** panel of normals (PoN) containing germline and artifactual sites for use with Mutect2. Step 1. Run Mutect2 in tumor-only mode for each normal sample. 12345678~/biosoft/GATK4.0/gatk-4.0.5.1/gatk Mutect2 \-R ~/reference/genome/gatk_hg38/Homo_sapiens_assembly38.fasta \-I HG00190.bam \-tumor HG00190 \--disable-read-filter MateOnSameContigOrNoMappedMateReadFilter \-L interval_list \--max-map-distance 0 \ ## 防止在产生的vcf文件中出现MNPS现象-O 3_HG00190.vcf.gz 其中的interval_list来自于安捷伦公司的官网，使用的具体bed文件为：S07604514_Padded.bed或者是可以通过CCDS文件自己制作interval_list 首先在CCDS官网（ftp://ftp.ncbi.nlm.nih.gov/pub/CCDS/current_human）下载文件 将txt文件转换为bed形式 1cat CCDS.20160908.txt |grep -w -v Withdrawn|perl -alne '&#123;/\[(.*?)\]/;next unless $1;$gene=$F[2];$exons=$1;$exons=~s/\s//g;$exons=~s/-/\t/g;print "$F[0]\t$_\t$gene" foreach split/,/,$exons;&#125;'|sort -u |bedtools sort -i &gt;exon_probe.hg38.gene.bed 制作interval_list 123456789101112131415cat exon_probe.hg38.gene.bed | awk '&#123;print "chr"$0&#125;' &gt;hg38.chr.bedpicard BedToIntervalList \ I=hg38.chr.bed \ O=hg38.interval_list\ SD=/data/reference/GRCh38/GRCh38.primary_assembly.genome.dict gatk4 PreprocessIntervals -L hg38.interval_list --sequence-dictionary /data/reference/GRCh38/GRCh38.primary_assembly.genome.dict --reference /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa --padding 100 --bin-length 0 --interval-merging-rule OVERLAPPING_ONLY --output hg38.Padded.interval_list Step 2. Create a GenomicsDB from the normal Mutect2 calls.由于在第一步产生的vcf文件中有多行对应一个位点，所以在进行Step2之前要先将这些同一位点的行合并起来 1234567bcftools norm \ -m +any --do-not-normalize \ delta_af-only-gnomad_Hg19toGRCh38.vcf.gz \ -Oz -o zeta_af-only-gnomad_Hg19toGRCh38.vcf.gz## 为norm之后的新vcf文件创建索引gatk IndexFeatureFile \ -F cohort.vcf.gz 123456## step.2 gatk GenomicsDBImport -R reference.fasta -L intervals.interval_list \ --genomicsdb-workspace-path pon_db \ -V normal1.vcf.gz \ -V normal2.vcf.gz \ -V normal3.vcf.gz Step 3. Combine the normal calls using CreateSomaticPanelOfNormals. 1gatk CreateSomaticPanelOfNormals -R reference.fasta -V gendb://pon_db -O pon.vcf.gz ** 对tumor和matched normal进行calls somatic variants ** 123456789101112131415for sample in *C*.bam; do base=$&#123;sample%-*&#125;; gatk4 Mutect2 -R /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa -I "$base"-N.sorted.markdup.recal.bam -I $sample -tumor "$base"-C -normal "$base"-N -pon ./Mutect2_interval/pon.vcf.gz --germline-resource ../../reference/somatic-hg38_af-only-gnomad.hg38.vcf.gz --af-of-alleles-not-in-resource 0.0000025 -disable-read-filter MateOnSameContigOrNoMappedMateReadFilter -L ../../reference/S07604514_hs_hg38/S07604514_hs_hg38/S07604514_Padded.bed -O ./Mutect2_interval/Mutect2_output/"$base"_somatic_m2.vcf.gz -bamout ./Mutect2_interval/Mutect2_output/"$base"_tumor_normal_m2.bam;done ** 使用GetPileupSummaries工具来计算肿瘤样本的污染情况 ** 先使用Selectvariants工具生成包含commom biallelic variants的.vcf文件 1234567 gatk SelectVariants \-R ~/reference/genome/gatk_hg38/Homo_sapiens_assembly38.fasta \-V af-only-gnomad.hg38.vcf.gz \--select-type-to-include SNP \--restrict-alleles-to BIALLELIC \-O af-only-gnomad.hg38.SNP_biallelic.vcf.gz 接着用GetPileupSummaries计算resource site位点上的count数，这样并不是计算所有的位点，而是用AF参数过滤后的（大于0.01并小于0.2），同时也可以用 -L 参数指定区域，分别对Normal和Tumor样本计算 12345~/biosoft/GATK4.0/gatk-4.0.5.1/gatk GetPileupSummaries \-I Normal_blood.allready.bam \-L ../exon_150bp.list \-V ~/annotation/GATK/resources/bundle/hg38/af-only-gnomad.hg38.SNP_biallelic.vcf.gz \-O Normal_blood.pileups.table 总之最后通过计算出的污染比例（XXX.calculatecontamination.table文件中）来过滤掉somatic variant中的一些可能是污染导致的假阳性突变 1234gatk CalculateContamination \-I Primary-IIIG.pileups.table \-matched Normal_blood.pileups.table \-O Primary-IIIG.calculatecontamination.table 使用Muect2新流程进行calling somatic mutation首先需要创建PON，步骤与旧版的相同，接下买的操作如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950## First, run Mutect2 with the --f1r2-tar-gz argument. This creates an output with raw dataif [ 1 -eq 1 ];then workdir=/media/data1/YJ/CRC/tag_duplicates/tag-dup_rec/;outdir=/media/data1/YJ/CRC/tag_duplicates/tag-dup_rec/Mutect2_interval/Mutect2_new_withnewPON_matchedmodel/;refdir=/media/data1/YJ/CRC/reference/;cd $workdir;for sample in *C*.bam;do base=$&#123;sample%%-*&#125;; gatk4 Mutect2 -R /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa \ -L /media/data1/YJ/CRC/reference/S07604514_hs_hg38/S07604514_hs_hg38/S07604514_Padded.bed \ -I $sample \ -I "$base"-N.sorted.markdup.recal.bam \ -tumor "$base"-C \ -normal "$base"-N \ -germline-resource "$refdir"somatic-hg38_af-only-gnomad.hg38.vcf.gz \ -pon "$workdir"Mutect2_interval/pon_withgnomAD.vcf.gz \ --f1r2-tar-gz "$outdir""$base".f1r2.tar.gz \ -O "$outdir""$base".unfiltered.vcf;done;fi## Next, pass this raw data to LearnReadOrientationModel:if [ 1 -eq 1 ] ;then workdir=/media/data1/YJ/CRC/tag_duplicates/tag-dup_rec/Mutect2_interval/Mutect2_new_withnewPON_matchedmodel; cd $workdir; for sample in *.tar.gz; do base=$&#123;sample%%.*&#125;; gatk4 LearnReadOrientationModel -I $sample -O "$base"_read-orientation-model.tar.gz; done;fi## Finally, pass the learned read orientation model to FilterMutectCallswith the -ob-priors argument:if [ 1 -eq 1 ];then workdir=/media/data1/YJ/CRC/tag_duplicates/tag-dup_rec/Mutect2_interval/Mutect2_new_withnewPON_matchedmodel; refdir=/data/reference/GRCh38; cd $workdir for sample in *.unfiltered.vcf; do base=$&#123;sample%%.*&#125;; gatk4 FilterMutectCalls -V $sample \ --ob-priors "$base"_read-orientation-model.tar.gz \ -R "$refdir"/GRCh38.primary_assembly.genome.fa \ -O "$base"_filtered.vcf done;fi 最终获得利用FilterMutectCalls过滤过的filtered.vcf文件然后得到通过过滤的vcf文件 12345678910##filter the .filtered.vcf and annotation it with gene_based modelif [ 1 -eq 1 ];then for sample in *_filtered.vcf; do base=$&#123;sample%%_*&#125;; grep -v "#" $sample | awk '$7=="PASS"' &gt; "$base"_filtered_PASS.cvf; done;fi 可以通过VariantAnnotator对vcf文件进行dbsnp注释 1234567891011121314##Annotation vcf with dbsnp if [ 1 -eq 0 ]; then for sample in *_filtered.vcf; do base=$&#123;sample%%_*&#125;; gatk4 VariantAnnotator -R /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa \ -I ../../"$base"-C.sorted.markdup.recal.bam \ -V $sample \ --output "$base"_filtered_dbsnp.vcf \ -A Coverage \ --dbsnp /media/data1/YJ/CRC/reference/common_all_20180418_dbsnp_151_hg38.vcf ; done fi 注释完的vcf文件中多了一列ID的信息，里面有snp号，可以通过SNPedia网站查看snp的具体信息 使用不同的方法进行annotationANNOVAR在ANNOVAR官网下载最新版本的软件 123456789101112131415161718## 下载适合的数据库(下载的数据储存在GRCH38文件夹中) annotate_variation.pl -downdb -buildver hg19 -webfrom annovar refGene GRCH38/## 利用 ANNOVAR进行基于gene的annotationif [ 1 -eq 1 ];then sofadir=/media/data1/YJ/CRC/softwave/annovar; for sample in *filtered_PASS.vcf; do base=$&#123;sample%%.*&#125;; $sofadir/convert2annovar.pl -format vcf4 $sample -outfile "$base".avinput; $sofadir/annotate_variation.pl -out "$base" \ -build hg38 \ "$base".avinput \ --geneanno -dbtype refGene \ $sofadir/GRCH38/ done;fi 结果会得到两个文件： 在外显子位点的注释：s0020487998_filtered_PASS.exonic_variant_function 在所有位点的注释：s0020487998_filtered_PASS.variant_functionXX.variant_function文件一般关心前两列，后面几列均是变异位点的一些信息第一列是变异所在基因组的位置，如：exonic、splicing、ncRNA等；其优先级：exonic = splicing &gt; ncRNA&gt; &gt; UTR5/UTR3 &gt; intron &gt; upstream/downstream &gt; intergenic第二列信息则给突变位点所在的基因名称（如果突变在exonic/intronic/ncRNA），或者给出临近基因的名称 ClinEff在ClinEff的官网下载最新版本的软件已经相关的database 12tar -xvf clinEff.1.0h.tgztar -xvf clinEff_db38_v1.0.tgz SnpEff在SnpEff的官网下载最新版本的软件 1234567891011121314151617wget http://sourceforge.net/projects/snpeff/files/snpEff_latest_core.zipunzip snpEff_latest_core.zip#查看homo_sapiens相关的数据库java -jar snpEff.jar databases | grep "Home_sapiens" | cut -f 1,2GRCh37.75 Homo_sapiensGRCh38.86 Homo_sapienshg19 Homo_sapiens (UCSC)hg19kg Homo_sapiens (UCSC KnownGenes)hg38 Homo_sapiens (UCSC)hg38kg Homo_sapiens (UCSC KnownGenes)testHg19ChrM Homo_sapiens (UCSC)#由于我们之前是使用的GRCh38的基因组，所以我们就下载GRCh38.86java -jar snpEff.jar download GRCh38.86#注释java -Xmx4g -jar /media/data1/YJ/CRC/softwave/snpEff/snpEff.jar GRCh38.86 s0020347988_filtered_PASS.vcf &gt; s0020347988_filtered_PASS.snpEff.vcf 输出的vcf文件，在输入vcf文件的基础上添加了一些tag：ANN、LOF、NMD因此我可以将这个vcf格式文件稍微处理下，保留原来的vcf文件的前5列，再加上ANNtag形成一个新文件来查看 12345for sample in *PASS*;do base=$&#123;sample%%_*&#125;; perl -alne 'next if $_ =~ /^#/;$F[7] =~ /(ANN=\S+)/;print "$F[1]\t$F[2]\t$F[3]\t$F[4]\t$F[5]\t$1"' $sample &gt; "$base".anntag.vcfdone vcf文件的内容如下： 1234567829275630 . C T . ANN=T|stop_gained|HIGH|PTPRU|ENSG00000060656|transcript|ENST00000345512.7|protein_coding|8/31|c.1327C&gt;T|p.Gln443*|1456/4470|1327/4341|443/1446||, T|stop_gained|HIGH|PTPRU|ENSG00000060656|transcript|ENST00000373779.7|protein_coding|8/30|c.1327C&gt;T|p.Gln443*|1456/5579|1327/4311|443/1436||, T|stop_gained|HIGH|PTPRU|ENSG00000060656|transcript|ENST00000428026.6|protein_coding|8/30|c.1327C&gt;T|p.Gln443*|1423/5550|1327/4302|443/1433||, T|stop_gained|HIGH|PTPRU|ENSG00000060656|transcript|ENST00000460170.2|protein_coding|8/31|c.1327C&gt;T|p.Gln443*|1333/5337|1327/4323|443/1440||, T|upstream_gene_variant|MODIFIER|PTPRU|ENSG00000060656|transcript|ENST00000415600.6|processed_transcript||n.-3921C&gt;T|||||3921|, T|non_coding_transcript_exon_variant|MODIFIER|PTPRU|ENSG00000060656|transcript|ENST00000527027.1|processed_transcript|3/3|n.564C&gt;T||||||; LOF=(PTPRU|ENSG00000060656|11|0.36); NMD=(PTPRU|ENSG00000060656|11|0.36) 往往一个突变条目对应着多个annotation报告，主要有一下几种原因： 一个突变位点可以影响多个基因，比如一个突变可能是一个基因的DOWNSTREAM同时可能是另一个基因的UPSTREAM 在复杂的基因组中，一个基因可能对应着多个转录组，snpeff会指出受该突变影响的每一个转录本 一条vcf记录可能包含不止一种突变123456789#CHROM POS ID REF ALT QUAL FILTER INFO1 889455 . G A,T . . ANN=A|stop_gained|HIGH|NOC2L|ENSG00000188976|transcript|ENST00000327044|protein_coding|7/19|c.706C&gt;T|p.Gln236*|756/2790|706/2250|236/749|| ,T|missense_variant|MODERATE|NOC2L|ENSG00000188976|transcript|ENST00000327044|protein_coding|7/19|c.706C&gt;A|p.Gln236Lys|756/2790|706/2250|236/749|| ,A|downstream_gene_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000487214|processed_transcript||n.*865C&gt;T|||||351| ,T|downstream_gene_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000487214|processed_transcript||n.*865C&gt;A|||||351| ,A|downstream_gene_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000469563|retained_intron||n.*878C&gt;T|||||4171| ,T|downstream_gene_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000469563|retained_intron||n.*878C&gt;A|||||4171| ,A|non_coding_exon_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000477976|retained_intron|5/17|n.2153C&gt;T|||||| ,T|non_coding_exon_variant|MODIFIER|NOC2L|ENSG00000188976|transcript|ENST00000477976|retained_intron|5/17|n.2153C&gt;A||||||;LOF=(NOC2L|ENSG00000188976|6|0.17);NMD=(NOC2L|ENSG00000188976|6|0.17) 当存在多种effect时，effect排序的依据： 根据Putative impact：具有更高等级的putative impact将会排在前面 Effect type：有害的effect会排在前面 Canonical trancript before non-canonical. Marker genomic coordinates (e.g. genes starting before first). ANNtag将注释信息以“|”分割，每个filed有其对应的信息： Allele（or ALT）：突变的碱基 Annotation：表示突变的类型，当有多种类型存在时，用@连接，可以在 The Sequence ontology官网查询每一种突变类型的具体含义 Putative_impact：对突变的影响进行的预测，有4个程度 1.HIGH：突变对蛋白的影响很大，可能导致蛋白质截短、功能丧失或引发无意义的介导衰退。eg.stop_gained, frameshift_variant2.MODERATE：一种可能改变蛋白质有效性的无破坏性突变 eg.missense_variant, inframe_deletion3.LOW：几乎是没害的，不会改变蛋白质的性能，eg.synonymous_variant4.MODIFIER:通常是影响非编码基因的突变或非编码区的突变，对其很难预测它的影响。eg.exon_variant, downstream_gene_variant Gene ID：使用ENSEMBL id Feature type：表示突变所在区域的类型，比如transcript, motif, miRNA等 Feature ID ：表示Feature type对应的id Transcript biotype ：关于transcript的类型，根据ENSEMBL biotypes来定义， Rank/total：1/1表示Exon or Intron rank / total number of exons or introns，前面的1表示这个突变是在第1个exon上（因为annatation已经给出了这个是突变是在exon上），后面的11表示这个突变所在的transcript总共有1个exon HGVS.c ：n.321T&gt;C表示Variant using HGVS notation (DNA level) HGVS.p：如果突变发生在ｃｏｄｉｎｇ区域，根据HGVS标记法在蛋白质水平表示突变类型。p.Gln443*表示443位的Gln氨基酸突变成了终止密码子 cDNA_position/cDNA_len:cDNA的位置以及，cDNA的长度 CDS_position / CDS_len: Position and number of coding bases (one based includes START and STOP codons). Protein_position / Protein_len: Position and number of AA (one based, including START, but not STOP). 至于LOF和NMD标签这是表示：Loss of funcation(LOF) and nonsense-mediated decay(NMD) 预测 ，分析相关的effect是否可以产生LOF或者是NMD影响 NMD：无义介导的mRNA降解作为真核细胞中重要RNA监控机制，识别并降解开放阅读框中含有提前终止密码子（premature termination codon，PTC）的mRNA，以避免因截短的蛋白产物积累对细胞造成毒害. NMD还调控正常生理基因的表达，暗示其在真核细胞中扮演重要角色. LOF和NMD标签的形式如下： Gene：gene的名称 ID：gene id，通常是ENSEMBL id Num_transcripts：gene中的转录本的数量 percent_affected：被改突变多影响的转录本的比例 使用VarScan2 Somatic来call somatic（没有写完）环境搭建1 创建文件夹 1mkdir -p input reference scripts temp 2 安装conda并使用conda创建env 12345678910# 添加信号源conda config --add channels rconda config --add channels defaultsconda config --add channels conda-forgeconda config --add channels bioconda# 创建envconda create --name altwes gatk R samtools trimmomatic picard bam-readcountvarscan# 激活envconda activate altwes 3 安装GATK3.8 1234567WES=/cd $WES/scriptswget -r -np -nd \-O gatk3.tar.bz2 \’https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive&amp;version=3.8-0-ge9d806836’bunzip2 gatk3.tar.bz2tar -xf gatk3.tar Detect MSI数据预处理局部重比对接下来是局部区域重比对，通常也叫Indel局部区域重比对。有时在进行这一步骤之前还有一个merge的操作，将同个样本的所有比对结果合并成唯一一个大的BAM文件，merge的例子如下： 1samtools merge &lt;out.bam&gt; &lt;in1.bam&gt; [&lt;in2.bam&gt;...&lt;inN.bam&gt;] 局部重比对的目的是将BWA比对过程中所发现有潜在序列插入或者序列删除（insertion和deletion，简称Indel）的区域进行重新校正。这个过程往往还会把一些已知的Indel区域一并作为重比对的区域GATK4.0中没有分离出单独的局部比对脚本。只能使用3.X版本中的脚本,这里包含了两个步骤： 第一步，RealignerTargetCreator ，目的是定位出所有需要进行序列重比对的目标区域 第二步，IndelRealigner，对所有在第一步中找到的目标区域运用算法进行序列重比对，最后得到捋顺了的新结果。12345678910111213141516java -jar /path/to/GenomeAnalysisTK.jar \ -T RealignerTargetCreator \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.bam \ -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \ -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ -o sample_name.IndelRealigner.intervalsjava -jar /path/to/GenomeAnalysisTK.jar \ -T IndelRealigner \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.bam \ -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \ -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ -o sample_name.sorted.markdup.realign.bam \ --targetIntervals sample_name.IndelRealigner.intervals 这一步中我们实际使用到的known indel集为： Homo_sapiens_assembly38.known_indels.vcf Mills_and_1000G_gold_standard.indels.hg38.vcf 这些文件可以方便的在GATK bundle里面下载（ftp://ftp.broadinstitute.org/bundle/hg38/） 后面的变异检测使用GATK，而且使用GATK的HaplotypeCaller模块或者是Mutect2模块时，当这个时候才可以减少这个Indel局部重比对的步骤。 重新矫正碱基质量值 分为两步: 123456789101112 gatk BaseRecalibrator \ -I my_reads.bam \ -R reference.fasta \ --known-sites sites_of_variation.vcf \ --known-sites another/optional/setOfSitesToMask.vcf \ -O recal_data.table## 用到的已知编译位点common_all_20180418_dbsnp_151_hg38.vcfHomo_sapiens_assembly38.known_indels.vcfMills_and_1000G_gold_standard.indels.hg38.vcf下载地址（GATK的bundle以及1000genomic官网）：ftp://ftp.broadinstitute.org/bundle/hg38/https://ftp.ncbi.nlm.nih.gov/snp/organisms/ 12345gatk ApplyBQSR \ -R reference.fasta \ -I input.bam \ --bqsr-recal-file recalibration.table \ -O output.bam MSI鉴定通过以上步骤我么就获得了干净的bam文件可以用于下一步的MSI鉴定分析。在此次鉴定中一共使用到了三种MSI鉴定软件用于鉴定我们的tumer-normal paired样本 ** MSIsensor ** MSIsensor通过分别判断每个微卫星位点的稳定性，然后以不稳定微卫星位点的比例作为MSI得分。MSIsensor需要基于配对的肿瘤-正常样本进行MSI的判定。 首先, 对于在肿瘤和正常样本中测序深度均大于等于20的微卫星位点, 计算其等位基因的分布信息; 其次, 通过卡方检验比较肿瘤和正常样本的相同微卫星位点的等位基因分布, 若显著不同, 则认为该微卫星位点不稳定; 最后统计不稳定位点的比例, 若该比例超过阈值, 则判定为MSI-H,这个阈值一般是3.5% 1234#重参考基因组中获取微卫星位点msisensor scan -d reference.fa -o microsatellites.list#MSI扫描msisensor msi -d microsatellites.list -n normal.bam -t tumor.bam -e bed.file -o output.prefix 结果显示3个样本都是MSS** MANTIS ** MANTIS软件使用Python编辑，最新的版本2.7.1,但是Python3.0版本也是支持的下载地址：https://github.com/OSU-SRLab/MANTIS类似于MSIsensor, MANTIS也获得了肿瘤-正常配对样本在每个微卫星位点的等位基因分布信息; 与MSIsensor不同的是, 对于每个微卫星位点, MANTIS把上述两组数据看作两个向量, 定义这两个向量的 L1范数为样本中该位点的稳定程度, 对所有位点的L1范数求平均值即为样本的MSI得分 123456#从参考基因组中获得微卫星位点./RepeatFinder –i /path/to/genome.fasta -o /path/to/loci.bed#对上一步获得的微卫星位点根据exon测序实验室中的target位置信息进行过滤bedtools intersect -a MANTIS.loc.bed -b hg38_Regions.bed &gt; MANTIS_filter.bed#进行MSI鉴定python mantis.py --bedfile /path/to/loci.bed --genome /path/to/genome.fasta -n /path/to/normal.bam -t /path/to/tumor.bam -o /path/to/output/file.txt 对于外显子测序，可使用以下不太严格的参数设置： 1234-mrq 20.0-mlq 25-mlc 20-mrr 1 结果显示3个样本都是MSS ** VisualMSI ** 通过模仿PCR来检测MSI。VisualMSI从参考基因组中提取PCR adapter，并将其map到测序read中。加入adapter可以成功的map到raed上，将计算这些adapter的插入长度。VisualMSI统计计算这些长度的分布情况。 1visualmsi -i tumor.sorted.bam -n normal.sorted.bam -r hg19.fasta -t targets/msi.tsv 关于target file VisualMSI官网提供的参考文件为： 123456#CHROM POSITION NAMEchr4 55598216 BAT25chr2 47641568 BAT26chr14 23652365 NR-21chr11 102193518 NR-27chr2 95849372 NR-24 但是他是根据hg19参考基因组来做的，我们需要将它转换为hg38参考基因组的,可以利用UCSC的liftover工具 下载注释文件 https://hgdownload-test.gi.ucsc.edu/goldenPath/hg19/liftOver/ 中的hg19ToHg38.over.chain.gz文件 输入文件需要是标准的bed格式 12345678910必须的字段：- 染色体- 起始位置（第一个氨基酸为0）- 终止位置chr1 213941196 213942363chr1 213942363 213943530chr1 213943530 213944697chr2 158364697 158365864chr2 158365864 158367031可选字段：name、score、strand.... 进行转换1../softwave/liftOver CRC_cancer-Specific_marker.bed hg19ToHg38.over.chain.gz hg38_CRC_cancer-Specific_marker.tsv unmap 不知道为什么大多数的position都显示能通过过滤标准的reads特别的少，不能通过质量控制，这个软件不适合？还是说参数设置不对？** mSINGS ** 1) Catalog microsatellites present in your host genome. There are a number of algorithms available to to this, but one we have found particularly easy to use is MISA (http://pgrc.ipk-gatersleben.de/misa/).2) Limit the list of microsatellites to those which are present in your capture design. This can be done by converting the location of identified microsatellites from step #1 to BED file format, converting the coordinates of your capture design to BED file format, and using the BEDTOOLS intersect function to find where these two files overlap (http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html).3) Generate support files and a baseline file from no fewer than 10-20 specimens, as detailed in the readme.4) Examine the baseline file. Identify any loci which have an average peak count of 1.0 / standard deviation of 0.0, as these loci may cause artifacts. These should be removed from the baseline file and the original locus BED file. The intervals file will need to be re-generated with the edited input BED file.5) Perform mSINGS analysis of the individual files that you used to generate your baseline with the edited files. Any remaining loci which are called as unstable in a measurable fraction (~10% or more) of known negative specimens should also be dropped due to their potential to cause artifacts. Remove these loci and edit files as in step #46) Optional - if you have access to a number of known MSI positive specimens (10 or more), it can be helpful to analyze them at this point. Discriminatory power of mSINGS can be improved if uninformative loci (those which are not unstable in 1 or more known MSI specimens) are removed from the panel. You may remove uninformative loci as in step #4. If these specimens are not available, be aware that an empirically-determined cutoff for discriminating MSS form MSI-H specimens may need to be established for your assay. mSINGs的使用流程： 1234567891011##由于mSINGs是基于Python 2.7写的，所以首先要将shell切换到Python 2.7环境source activate py2.7git clone https://bitbucket.org/uwlabmed/msings.gitcd msingsbash ./dev/bootstrap.sh##切换到msings-env环境source msings-env/bin/activate##利用MANTIS软件中的获得的filter_bed（capture.bed的交集）文件进行下一步分析./scripts/create_intervals.sh ../../reference/mSINGS/MANTIS_filtered.bed##创建msi_bed文件./scripts/create_baseline.sh ../../reference/mSINGS/normal_bamlist ../../reference/mSINGS/MANTIS_filtered.msi_intervals ../../reference/mSINGS/MANTIS_filtered.bed /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa 上面的到MSI_BASELINE.txt文件，筛选到其中Standard_Deviation=0的Position，这些Position被认为是没有意义的误差，然后与之前获得的bed文件取交集，获得有意义的bed文件对normal样本进行MSI鉴定，对bed文件进行进一步的验证筛选 12345./scripts/run_msings.sh ../../reference/mSINGS/cancer_bamlist ../../reference/mSINGS/MANTIS_filtered.filterSD.msi_intervals ../../reference/mSINGS/MANTIS_filtered.filterSD.bed /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa ../../reference/mSINGS/MSI_BASELINE_filterSD.txt 根据获的Combined_MSI.txt结果，再从MSI_BASELIEN_filterSD.txt文件中删除掉在instable_num/total_num &gt; 10% 的position，同样的bed文件和interval也做相同的处理,得到最终的MSI_BASELINE_filterSD_checknorm.txt最后进行cancer样本的MSI鉴定 ./scripts/run_msings.sh ../../reference/mSINGS/cancer_bamlist ../../reference/mSINGS/MANTIS_filtered.filterSD.checknorm.msi_intervals ../../reference/mSINGS/MANTIS_filtered.filterSD.checknorm.bed /data/reference/GRCh38/GRCh38.primary_assembly.genome.fa ../../reference/mSINGS/MSI_BASELINE_filterSD_checknorm.txt结果显示3个样本都是MSS的]]></content>
      <categories>
        <category>Bioinformatic_project</category>
      </categories>
      <tags>
        <tag>exome</tag>
      </tags>
  </entry>
</search>
